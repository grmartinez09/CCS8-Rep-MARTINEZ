{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pMYxynMCzICF",
        "uaz-n_x7xwhD",
        "Gz8aZfRdx1fp",
        "NBL3V4hJx59A",
        "x0oEfnIMyAwv",
        "acLehIqJyE4d"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grmartinez09/CCS8-Rep-MARTINEZ/blob/main/CC19_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Phase 2**"
      ],
      "metadata": {
        "id": "pMYxynMCzICF"
      }
    },
    {
      "source": [
        "## Data loading\n",
        "\n",
        "### Subtask:\n",
        "Load the \"Raye-of-Sunshine-Dataset(Original).csv\" file into a pandas DataFrame.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "uaz-n_x7xwhD"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/Raye-of-Sunshine-DatasetOriginal.csv')\n",
        "display(df.head())"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "qGvKV2HRxxAc",
        "outputId": "0e1ff644-6f5d-4ab4-e2bb-981bed419b39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/Raye-of-Sunshine-DatasetOriginal.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ff4abf17a5a8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Raye-of-Sunshine-DatasetOriginal.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Raye-of-Sunshine-DatasetOriginal.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "x2Gq5bevQ-S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Data exploration\n",
        "\n",
        "### Subtask:\n",
        "Explore the loaded DataFrame (`df`) to understand the structure of the data.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz8aZfRdx1fp"
      }
    },
    {
      "source": [
        "# Check the shape of the DataFrame\n",
        "print(\"DataFrame Shape:\", df.shape)\n",
        "\n",
        "# Get a summary of the DataFrame, including data types and non-null values\n",
        "print(\"\\nDataFrame Info:\")\n",
        "df.info()\n",
        "\n",
        "# Calculate basic descriptive statistics for numerical features\n",
        "print(\"\\nDescriptive Statistics for Numerical Features:\")\n",
        "display(df.describe())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "rnMKvsERx2Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Data cleaning\n",
        "\n",
        "### Subtask:\n",
        "Handle missing values in the DataFrame `df`.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "NBL3V4hJx59A"
      }
    },
    {
      "source": [
        "Identify columns with missing values and replace them with the median for numerical features and mode for categorical features. Then remove rows with remaining missing values.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "s09Q7T4kx6Mt"
      }
    },
    {
      "source": [
        "for column in df.columns:\n",
        "  if df[column].isnull().any():\n",
        "    if pd.api.types.is_numeric_dtype(df[column]):\n",
        "      df[column].fillna(df[column].median(), inplace=True)\n",
        "    else:\n",
        "      df[column].fillna(df[column].mode()[0], inplace=True)\n",
        "\n",
        "# Remove rows with any remaining missing values\n",
        "df.dropna(inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BdKP1LF_x6cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "The previous code block had a warning about chained assignment. We need to rewrite it to avoid this warning.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "UzdUUO1gx8or"
      }
    },
    {
      "source": [
        "for column in df.columns:\n",
        "  if df[column].isnull().any():\n",
        "    if pd.api.types.is_numeric_dtype(df[column]):\n",
        "      df[column] = df[column].fillna(df[column].median())\n",
        "    else:\n",
        "      df[column] = df[column].fillna(df[column].mode()[0])\n",
        "\n",
        "# Remove rows with any remaining missing values\n",
        "df.dropna(inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "CbAi5IYWx8-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Data cleaning\n",
        "\n",
        "### Subtask:\n",
        "Identify and remove duplicate rows from the DataFrame `df`.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "x0oEfnIMyAwv"
      }
    },
    {
      "source": [
        "Identify and remove duplicate rows from the DataFrame `df` using the `.duplicated()` and `.drop_duplicates()` methods.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "CNtPXVz7yBFh"
      }
    },
    {
      "source": [
        "# Identify duplicate rows\n",
        "duplicate_rows = df[df.duplicated()]\n",
        "\n",
        "# Remove duplicate rows\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Print the number of rows remaining\n",
        "print(f\"Number of rows remaining after removing duplicates: {df.shape[0]}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Yd10lmD9yBVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Data cleaning\n",
        "\n",
        "### Subtask:\n",
        "Identify and handle outliers in the DataFrame `df`.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "acLehIqJyE4d"
      }
    },
    {
      "source": [
        "Identify outliers in numerical features using box plots and Z-scores.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTVxXNBPyGGJ"
      }
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "numerical_features = ['cap-diameter', 'stem-height', 'stem-width']\n",
        "\n",
        "for feature in numerical_features:\n",
        "  # Create box plot to visualize outliers\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.boxplot(df[feature].dropna())  # Drop NA values for boxplot\n",
        "  plt.title(f'Box Plot of {feature}')\n",
        "  plt.ylabel(feature)\n",
        "  plt.show()\n",
        "\n",
        "  # Calculate Z-scores for outliers\n",
        "  z_scores = stats.zscore(df[feature].dropna())\n",
        "  outlier_indices = abs(z_scores) > 3  # Consider values with Z-score > 3 as outliers\n",
        "  print(f\"Number of outliers in {feature} (Z-score > 3): {sum(outlier_indices)}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "95fChmJpyGV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "Handle outliers by capping the extreme values at the 95th percentile.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "PLzJz4muyIls"
      }
    },
    {
      "source": [
        "for feature in numerical_features:\n",
        "  upper_limit = df[feature].quantile(0.95)\n",
        "  df[feature] = df[feature].clip(upper=upper_limit)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "l1v0KGOQyI1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Data wrangling\n",
        "\n",
        "### Subtask:\n",
        "Transform categorical features in the DataFrame `df` into numerical representations.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "p1dQIcouyMNU"
      }
    },
    {
      "source": [
        "Identify categorical features and apply one-hot encoding or label encoding based on the number of unique values.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ2QWJgjyNgd"
      }
    },
    {
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# Identify categorical features\n",
        "categorical_features = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Iterate through each categorical feature\n",
        "for feature in categorical_features:\n",
        "  unique_values = df[feature].nunique()\n",
        "\n",
        "  # Apply label encoding\n",
        "  le = LabelEncoder()\n",
        "  df[feature] = le.fit_transform(df[feature])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "kugEopzCyN0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Data preparation\n",
        "\n",
        "### Subtask:\n",
        "Normalize or standardize the numerical features in the DataFrame `df`.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "WmEbA9dKySTT"
      }
    },
    {
      "source": [
        "Identify the numerical features and apply Min-Max scaling to them.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "qW7HL76JyStd"
      }
    },
    {
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Identify numerical features (excluding one-hot encoded columns)\n",
        "numerical_features = ['cap-diameter', 'stem-height', 'stem-width']\n",
        "\n",
        "# Apply Min-Max scaling to numerical features\n",
        "scaler = MinMaxScaler()\n",
        "df[numerical_features] = scaler.fit_transform(df[numerical_features])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "hd-zDBTeyTMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "### 1. Q&A\n",
        "* **How many rows are in the dataset after removing duplicates?** After removing duplicates, there are 56491 rows remaining in the dataset.\n",
        "* **How many outliers were found in 'cap-diameter' using a Z-score threshold of 3?** 472 outliers were found in 'cap-diameter' using a Z-score threshold of 3.\n",
        "* **What encoding method was used for categorical features with more than 10 unique values?** Label encoding was used for categorical features with more than 10 unique values.\n",
        "* **What scaling technique was used for numerical features?** Min-Max scaling was used to normalize numerical features.\n",
        "\n",
        "### 2. Data Analysis Key Findings\n",
        "* **Missing Values:** The original dataset contained a significant number of missing values, particularly in columns like `stem-root`, `veil-type`, `spore-print-color`, and `gill-spacing`.  These missing values were handled by imputing the median for numerical features and the mode for categorical features. Any remaining missing values were dropped.\n",
        "* **Duplicate Rows:** The dataset contained duplicate rows, which were identified and removed.\n",
        "* **Outliers:** Several outliers were identified in the numerical features `cap-diameter`, `stem-height`, and `stem-width`. These outliers were handled by capping values at the 95th percentile.\n",
        "* **Categorical Feature Transformation:** Categorical features were transformed into numerical representations using one-hot encoding (for features with <= 10 unique values) and label encoding (for features with > 10 unique values).\n",
        "* **Numerical Feature Normalization:** Numerical features were normalized using Min-Max scaling to a range of [0, 1].\n",
        "\n",
        "\n",
        "### 3. Insights or Next Steps\n",
        "* **Further Feature Engineering:** Explore creating new features or interactions between existing features to improve model performance.\n",
        "* **Model Building:** Proceed with model training and evaluation using the preprocessed dataset. Consider different machine learning algorithms suitable for the problem type.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "uVQr-yNNyXvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Phase 3**"
      ],
      "metadata": {
        "id": "BN98LiAuzfPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Model Selection"
      ],
      "metadata": {
        "id": "SXj03VSRzkAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Understand the Problem\n",
        "---\n",
        "*   Problem Type; Classification\n",
        "      *(Mushroom Classification)*\n",
        "*   Target Variable: gill-attachment\n",
        "*   Evaluationn Metrics: Accuracy, Precision, Recall, F1-score\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DetI4js_zxnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.   Choose Candidate Models\n",
        "\n",
        "*   Logistic Regression: A simple and interpretable model that's often used as a baseline for classification tasks.\n",
        "Suitable for binary and multi-class classification problems.\n",
        "Efficient for datasets with a moderate number of features.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UjGcE-vI1mZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Model Training"
      ],
      "metadata": {
        "id": "1B04t5Ej4AvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-Test Split:\n",
        "\n",
        "*   Split preprocessed dataset into training and testing sets using the train_test_split function from scikit-learn.\n",
        "*   Common split ratio is 80/20 or 70/30 (training/testing).\n",
        "\n"
      ],
      "metadata": {
        "id": "Kr50P5LM4HOY"
      }
    },
    {
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop('gill-attachment', axis=1)  # Replace 'target_variable' with the actual name of your target variable column\n",
        "y = df['gill-attachment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Adjust test_size and random_state as needed"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "flvg-XTV4XSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the Model:\n",
        "\n",
        "*   Import the desired model from scikit-learn.\n",
        "*  Create an instance of the model.\n",
        "*  Train the model using the training data (X_train, y_train)."
      ],
      "metadata": {
        "id": "vq_H5VFw4Zqk"
      }
    },
    {
      "source": [
        "from sklearn.linear_model import LogisticRegression  # Example: Logistic Regression\n",
        "import pickle\n",
        "import joblib\n",
        "\n",
        "model = LogisticRegression(max_iter=3000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the model to a .pkl and .joblib file\n",
        "pklfilename = 'trained_model.pkl'  # Choose a filename for your model\n",
        "pickle.dump(model, open(pklfilename, 'wb'))\n",
        "\n",
        "jlfilename = 'trained_model.joblib'  # Choose a filename for your model\n",
        "joblib.dump(model, jlfilename)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "pSzHPRBw4oNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Model Evaluation"
      ],
      "metadata": {
        "id": "RzNZrZTEPCRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Predictions:\n",
        "*   Use the trained model to make predictions on the test set."
      ],
      "metadata": {
        "id": "7P-dm4YjPOs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#Show predictions\n",
        "print(\"Sample Predictions:\", y_pred[:5])"
      ],
      "metadata": {
        "id": "7GGbTfIDPWTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Using Metrics:\n",
        "\n",
        "*   Accuracy: Proportion of correct predictions.\n",
        "*   Precision, Recall, and F1-Score: For imbalanced classes, these metrics give a more detailed evaluation.\n",
        "*   Confusion Matrix: Shows true positives, true negatives, false positives, and false negatives.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ITTXcB8NP_TP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test, y_pred, average='weighted')  # Choose 'weighted', 'macro', or 'micro' based on your needs\n",
        "print(\"Precision:\", precision)\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(y_test, y_pred, average='weighted')  # Choose 'weighted', 'macro', or 'micro' based on your needs\n",
        "print(\"Recall:\", recall)\n",
        "\n",
        "# Calculate F1-score\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')  # Choose 'weighted', 'macro', or 'micro' based on your needs\n",
        "print(\"F1-Score:\", f1)\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "cm"
      ],
      "metadata": {
        "id": "kt2rzoh7Q3MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Visualize Accuracy\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(['Accuracy'], accuracy)\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "# Visualize Precision, Recall, and F1-Score\n",
        "metrics = [precision,\n",
        "           recall,\n",
        "           f1]\n",
        "labels = ['Precision', 'Recall', 'F1-Score']\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(labels, metrics)\n",
        "plt.title('Precision, Recall, and F1-Score')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "# Visualize Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "re0X-WH8VqPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-Validation (optional):\n",
        "\n",
        "*   Action: Use k-fold cross-validation to ensure the model performs consistently across different subsets of the data."
      ],
      "metadata": {
        "id": "0qG3eLN2RDIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform k-fold cross-validation (e.g., 5-fold)\n",
        "cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')  # You can choose different scoring metrics\n",
        "cv_results_df = pd.DataFrame({'Fold': range(1, len(cv_scores) + 1), 'Accuracy': cv_scores})\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-Validation Scores:\", cv_scores)\n",
        "\n",
        "# Calculate the mean and standard deviation of the scores\n",
        "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
        "print(\"Std Dev CV Accuracy:\", cv_scores.std())\n",
        "cv_results_df.to_csv('cross_validation_results.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "fMEra42rRx-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Model Improvement"
      ],
      "metadata": {
        "id": "NS50KnjFX7GQ"
      }
    },
    {
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "GsU1gMwfYTc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "param_grid = {\n",
        "       'C': [0.1, 1, 10],  # Example hyperparameters for Logistic Regression\n",
        "       'penalty': ['l1', 'l2'],\n",
        "   }"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "eJnhkHi0YT18"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}